{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy-Based Methods\n",
    "Whereas *Value-Based Methods* like Deep Q-Learning are obtaining an optimal policy $\\pi_*$ by trying to estimate the optimal action-value function, *Policy-Based Methods* directly learn the optimal policy.  \n",
    "Besides this simplification another advantage of a Policy-Based Method is the fact that it is able to handle either stochastic or continuous actions.  \n",
    "On the one hand Policy-Based Methods are using the *Monte Carlo* (MC) approach for the estimate of expected return:\n",
    "\n",
    "$ G_t = R_{t+1} + R_{t+2} + ... + R_T$, if the discount factor $\\gamma=1$\n",
    "\n",
    "As $G_t$ is estimated with the full trajectory this yields to a high *variance*, but to a low *bias*.  \n",
    "On the other hand Value-Based Methods are using the *Temporal Difference* (TD) approach to estimate the return:\n",
    "\n",
    "$ G_t = R_{t+1} + G_{t+1}$ , if $\\gamma=1$\n",
    "\n",
    "Here $G_{t+1}$ is the estimated total return an agent will obtain in the next state. As the estimate of $G_t$ is always depending on the estimate of the next state, the variance of these estimates is low but biased.  \n",
    "The pros of both methods can be combined in one single algorithm namely the Actor-Critic Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor-Critic Methods\n",
    "In Actor-Critic Methods one uses two function approximators (usually neural networks) to learn a policy (Actor) and a value function (Critic). The process looks as follows:  \n",
    "\n",
    "1) Observe state $s$ from environment and feed into the Actor.  \n",
    "2) The output are action probabilities $\\pi(a|s;\\theta_\\pi)$. Select one action stochastically and feed back to the environment.  \n",
    "3) Observe next state $s'$ and reward $r$.  \n",
    "4) Use the tuple $(s, a, r, s')$ for the TD estimate $y=r + \\gamma V(s'; \\theta_v)$  \n",
    "5) Train the Critic by minimizing the loss $L=(y - V(s;\\theta_v)^2$.  \n",
    "6) Calculate the advantage $A(s,a) = r + \\gamma V(s'; \\theta_v) - V(s; \\theta_v)$.  \n",
    "7) Train the Actor using the advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Deterministic Policy Gradient\n",
    "The following section refers to [Lillicrap et al., 2016].  \n",
    "Deep Deterministic Policy Gradient (DDPG) combines the Actor-Critic approach with Deep Q-Learning. The actor function $\\mu(s;\\theta_\\mu)$ gives the current policy. It maps states to continuous deterministic actions. The critic $Q(s,a;\\theta_q)$ on the other hand is used to calculate action values and is learned using the Bellman equation. DDPG is also using a *replay buffer* and *target networks* which already helped to improve performance for Deep Q-Learning. In a finite replay buffer tuples of $(s, a, r, s')$ are stored and then batches are sampled from this buffer to apply for network updates. This tackles the issue of correlated tuples arrised from sequentially exploring the environment. Target networks are used to decouple the TD target from the current action value when performing neutwork updates. The target network is a copy of the Actor and Critic Network which are used to calculated the target. One approach is to update the weights of the target networks $\\theta'$ with the weights $\\theta$ of the Actor and Critic network periodically. An other approach is to perform *soft updates*:\n",
    "\n",
    "$ \\theta' \\leftarrow \\tau \\theta + (1-\\tau)\\theta'$ with $\\tau \\ll 1$\n",
    "\n",
    "In order to scale features *batch normalization* is being applied. This normalizes each dimension across the samples of the minibatch. An other important issue is handling exploration. By adding a noise process $N$ an exploration policy $\\mu'$ is constructed:\n",
    "\n",
    "$\\mu'(s_t) = \\mu(s_t;\\theta_{\\mu,t})+N$\n",
    "\n",
    "The DDPG process looks as follows:  \n",
    "1) Observe state $s$ from environment and feed to Actor.  \n",
    "2) Select action $a = \\mu(s;\\theta_\\mu) + N$ and feed back to environment.  \n",
    "3) Observe next state $s'$ and reward $r$.  \n",
    "4) Store transition $(s, a, r, s')$ in replay buffer and sample random minibatch of $n$ tuples. Calculate the TD estimate \n",
    "$y = r + \\gamma Q'(s', \\mu'(s';\\theta_\\mu');\\theta_q')$  \n",
    "5) Train the Critic by minimizing the loss \n",
    "$L=\\mathbb{E} \\big[\\big(y - Q(s,a;\\theta_q)\\big)^2\\big]$  \n",
    "6) Train Actor with policy gradient \n",
    "$\\mathbb{E} \\big[\\nabla_{\\theta_\\mu} Q(s,a;\\theta_q) | s=s_t, a=\\mu(s_t;\\theta_\\mu) \\big] = \\mathbb{E} \\big[\\nabla_a Q(s,a;\\theta_q)|s=s_t,a=\\mu(s_t) \\nabla_{\\theta_\\mu} \\mu(s;\\theta_\\mu)|s=s_t\\big] $  \n",
    "7) Update both target networks using soft update\n",
    "\n",
    "As one see, this is an off-policy algorithm because the policy which is evaluated uses action $a=\\mu'(s';\\theta_\\mu')$. This is different from the policy which selects action $a = \\mu(s;\\theta_\\mu) + N$. An other interesting aspect is that the Critic network has only one output node, which is the action value given the state and the action: $Q(s,a;\\theta_q)$ This is different to Deep Q-Learning where the Q-Network is mapping values to every possible (discrete) action node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from collections import deque\n",
    "\n",
    "from agent import Agent\n",
    "\n",
    "\n",
    "def initialize_env(unity_file):\n",
    "    # Initialize the environment\n",
    "    env = UnityEnvironment(file_name=unity_file, worker_id=3)\n",
    "\n",
    "    # Get default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # Get state and action spaces\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state_size = env_info.vector_observations.shape[1]\n",
    "    action_size = brain.vector_action_space_size\n",
    "    n_agents = len(env_info.agents)\n",
    "    \n",
    "    print('State size: ', state_size)\n",
    "    print('Action size: ', action_size)\n",
    "    print('Number of agents: ', n_agents)\n",
    "    \n",
    "    return env, brain_name, state_size, action_size, n_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(env, brain_name,\n",
    "         agent, n_agents,\n",
    "         n_episodes=2000, t_max=3000):\n",
    "    \"\"\"Deep Determinitic Policy Gradient.\n",
    "\n",
    "    Params\n",
    "    ======\n",
    "        env: unity environment object\n",
    "        brain_name (string): brain name of initialized environment\n",
    "        agent: initialized agent object\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        t_max (int): maximum timesteps in episode\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    best_score = -np.Inf\n",
    "    for e in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        score = np.zeros(n_agents)\n",
    "        #for _ in range(1, t_max):\n",
    "        while True:\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            agent.step(state[0], action[0], reward[0], next_state[0], done[0], learn=True)\n",
    "            agent.step(state[1], action[1], reward[1], next_state[1], done[1], learn=False)\n",
    "            score += reward\n",
    "            state = next_state\n",
    "            if np.any(done):\n",
    "                break\n",
    "\n",
    "        # Max score\n",
    "        max_score = np.max(score)\n",
    "        scores_window.append(max_score)\n",
    "        scores.append(max_score)\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(e, np.mean(scores_window), max_score), end=\"\")\n",
    "        if e % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(e, np.mean(scores_window)))\n",
    "        if np.mean(scores_window) > best_score:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            best_score = np.mean(scores_window)\n",
    "        if np.mean(scores_window)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(e-100, np.mean(scores_window)))\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores_dict):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    for key, scores in scores_dict.items():\n",
    "        scores_smoothed = gaussian_filter1d(scores, sigma=5)\n",
    "        plt.plot(np.arange(len(scores)), scores_smoothed, label=key)\n",
    "    plt.ylabel('smoothed Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size:  24\n",
      "Action size:  2\n",
      "Number of agents:  2\n",
      "Episode 100\tAverage Score: 0.02\tCurrent Score: 0.09\n",
      "Episode 200\tAverage Score: 0.05\tCurrent Score: 0.00\n",
      "Episode 300\tAverage Score: 0.38\tCurrent Score: 2.60\n",
      "Episode 311\tAverage Score: 0.51\tCurrent Score: 2.60\n",
      "Environment solved in 211 episodes!\tAverage Score: 0.51\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47e0b952b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "N = 10000\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = .99\n",
    "TAU = 1e-2\n",
    "LEARNING_RATE_ACTOR = 1e-4\n",
    "LEARNING_RATE_CRITIC = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "UPDATE_LOCAL = 1\n",
    "N_UPDATES = 1\n",
    "SEED = 40\n",
    "\n",
    "env, brain_name, state_size, action_size, n_agents = \\\n",
    "    initialize_env(\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "\n",
    "# Initialize agent\n",
    "agent = Agent(state_size, action_size,\n",
    "              n_agents, buffer_size=BUFFER_SIZE, \n",
    "              batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU,\n",
    "              lr_a=LEARNING_RATE_ACTOR, lr_c=LEARNING_RATE_CRITIC,\n",
    "              weight_decay=WEIGHT_DECAY, update_local=UPDATE_LOCAL,\n",
    "              n_updates=N_UPDATES, random_seed=SEED)\n",
    "\n",
    "# Train agent\n",
    "scores = ddpg(env, brain_name, agent, n_agents, n_episodes=N)\n",
    "\n",
    "plot_scores({'DDPG': scores})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further potentials to improve the RL agent\n",
    "The Multi-Agent DDPG (MADDPG) would be an alternative approach for this environment. It's core idea: during training phase both agent's critics are using information of every single agent whereas the actor selects action only based on each agent's observation. According to [Lowe et al., 2017] this approach should ease training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- Lillicrap, T., Hunt, J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wiestra, D., Continuous Control with Deep Reinforcement Learning, arXiv:1509.02971v5 [cs.LG] 29 Feb 2016  \n",
    "- Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., Mordatch, I., Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments, arXiv:1706.02275v3 [cs.LG] 7 Jun 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
